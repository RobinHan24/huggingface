{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "####数据准备\n",
    "# 定义字典\n",
    "zidian_x = '<SOS>,<EOS>,<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m'\n",
    "zidian_x = {word: i for i, word in enumerate(zidian_x.split(','))}\n",
    "\n",
    "zidian_xr = [k for k, v in zidian_x.items()]\n",
    "\n",
    "zidian_y = {k.upper(): v for k, v in zidian_x.items()}\n",
    "\n",
    "zidian_yr = [k for k, v in zidian_y.items()]\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 定义词集合\n",
    "    words = [\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'q', 'w', 'e', 'r',\n",
    "        't', 'y', 'u', 'i', 'o', 'p', 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k',\n",
    "        'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm'\n",
    "    ]\n",
    "\n",
    "    # 定义每个词被选中的概率\n",
    "    p = np.array([\n",
    "        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
    "        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26\n",
    "    ])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    # 随机选n个词\n",
    "    n = random.randint(30, 48)\n",
    "    x = np.random.choice(words, size=n, replace=True, p=p)\n",
    "\n",
    "    # 采样的结果就是x\n",
    "    x = x.tolist()\n",
    "\n",
    "    # y是对x的变换得到的\n",
    "    # 字母大写,数字取10以内的互补数\n",
    "    def f(i):\n",
    "        i = i.upper()\n",
    "        if not i.isdigit():\n",
    "            return i\n",
    "        i = 9 - int(i)\n",
    "        return str(i)\n",
    "\n",
    "    y = [f(i) for i in x]\n",
    "    y = y + [y[-1]]\n",
    "    # 逆序\n",
    "    y = y[::-1]\n",
    "\n",
    "    # 加上首尾符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "\n",
    "    # 补pad到固定长度\n",
    "    x = x + ['<PAD>'] * 50\n",
    "    y = y + ['<PAD>'] * 51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "\n",
    "    # 编码成数据\n",
    "    x = [zidian_x[i] for i in x]\n",
    "    y = [zidian_y[i] for i in y]\n",
    "\n",
    "    # 转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "    \n",
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return get_data()\n",
    "\n",
    "\n",
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=Dataset(),\n",
    "                                     batch_size=32,\n",
    "                                     drop_last=True,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, mask):\n",
    "    #b 句话  每句话50个词 每个词语编码成32维向量  4个头  每个头8维向量\n",
    "    # Q, K, V = [b,4,50,8] \n",
    "\n",
    "    # [b,4,50,8] * [b,40,8,50] -> [b,4,50,50]\n",
    "    # Q K 矩阵相乘 求每个词对其他所有词的注意力\n",
    "    score = torch.matmul(Q, K.permute(0 ,1,3,2))\n",
    "\n",
    "\n",
    "    # 除以每个头维数的平方根  做数值缩放\n",
    "    score /= 8 ** 0.5\n",
    "\n",
    "    # mask 遮掩 mask是一个布尔矩阵  mask为true的地方都替换成-inf  这样在计算softmax的时候 -inf会被压缩到0\n",
    "    # mask = [b, 1,50,50]\n",
    "    score = score.masked_fill_(mask,-float('-inf'))\n",
    "    score = torch.softmax(score, dim = 1)\n",
    "\n",
    "    #以注意力分数乘以V  得到最终的注意力结果\n",
    "    scoreValue = torch.matmul(score,V)\n",
    "\n",
    "    # 每个头计算的结果合一\n",
    "    # [b,4,50,8] -> [b,50,32]\n",
    "    scoreValue = scoreValue.permute(0,2,1,3).reshape(-1,50,32)\n",
    "\n",
    "    return scoreValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力计算层\n",
    "class MutilHead(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## 以下三个矩阵用来对 Q K V 做线性变化\n",
    "        self.fc_Q = torch.nn.Linear(32,32)\n",
    "        self.fc_K = torch.nn.Linear(32,32)\n",
    "        self.fc_V = torch.nn.Linear(32,32)\n",
    "\n",
    "        self.out_fc = torch.nn.Linear(32,32)\n",
    "\n",
    "        # 规范化之后,均值是0,标准差是1\n",
    "        # BN是取不同样本做归一化  视觉常用\n",
    "        # LN是取不同通道做归一化  NLP常用\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,elementwise_affine=True)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.1)\n",
    "    def forward(self,Q,K,V,mask):\n",
    "        # b句话 每句话50个词  每个词编码成32维向量\n",
    "        # Q K V = [b,50,32]\n",
    "\n",
    "        b = Q.shape[0]\n",
    "\n",
    "        # 保留下原始的Q 后面做短接要用\n",
    "\n",
    "        clone_Q = Q.clone()\n",
    "\n",
    "        #规范化\n",
    "        Q = self.norm(Q)\n",
    "        K = self.norm(K)\n",
    "        V = self.norm(V)\n",
    "\n",
    "        #线性运算  维度不变\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        K = self.fc_K(K)\n",
    "        Q = self.fc_Q(Q)\n",
    "        V = self.fc_V(V)\n",
    "\n",
    "        # 拆分成多个头  b句话 每句话50个词 每个词编码成32维向量  4个头  每个头分到8维向量\n",
    "        # [b,50,32] -> [b,4,50,8]\n",
    "\n",
    "        Q = Q.reshape(b,50,4,8).permute(0,2,1,3)\n",
    "        K = K.reshape(b,50,4,8).permute(0,2,1,3)\n",
    "        V = V.reshape(b,50,4,8).permute(0,2,1,3)\n",
    "\n",
    "        # 计算注意力\n",
    "        # [b,4,50,8] -> [b,50,32]\n",
    "        scoreValue = attention(Q,K,V,mask)\n",
    "\n",
    "        #计算输出  维度不变\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        scoreValue = self.dropout(self.out_fc(scoreValue))\n",
    "\n",
    "        # 短接 \n",
    "        scoreValue = clone_Q + scoreValue\n",
    "        return scoreValue\n",
    "\n",
    "import math\n",
    "# 位置编码层  编码并且添加位置信息\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # pos 是第几个词 i 是第几个维度  d_model是维度总数\n",
    "        def get_pe(pos,i,d_model):\n",
    "            fenmu = 1e4 ** (i / d_model)\n",
    "            pe = pos / fenmu\n",
    "\n",
    "            if i %2 ==0:\n",
    "                return math.sin(pe)\n",
    "            return math.cos(pe)\n",
    "\n",
    "        # 初始化位置编码矩阵\n",
    "        pe = torch.empty(50,32)\n",
    "        for i in range(50):\n",
    "            for j in range(32):\n",
    "                pe[i][j] = get_pe(i,j,32)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 定义为不更新的常量\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "\n",
    "        #词编码层\n",
    "        self.embed = torch.nn.Embedding(39,32) ###为什么是39  因为词典中一共39种词 words 36个 加上首尾符号表 加上pad 一共39个词\n",
    "        #初始化参数\n",
    "        self.embed.weight.data.normal_(0,0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #[8,50] -> [8,50,32]\n",
    "        embed = self.embed(x)\n",
    "\n",
    "        #词编码和位置编码相加\n",
    "        # [8,50,32] + [1,50,32] ->[8,50,32]\n",
    "        embed = embed + self.pe\n",
    "        return embed\n",
    "\n",
    "# 全连接输出层\n",
    "\n",
    "class FullyConnectOutput(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=32,out_features=64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=64,out_features=32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,elementwise_affine=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 保留下原始的x 后面要做短接用\n",
    "\n",
    "        clone_x = x.clone()\n",
    "        # 规范化\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 线性全连接运算\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        out = self.fc(x)\n",
    "\n",
    "        # 做短接\n",
    "        out = clone_x + out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####定义mask矩阵\n",
    "def mask_pad(data):\n",
    "    ##b 句话  每句话50个词  这里是还没embed的\n",
    "    # data = [b,50]\n",
    "    # 判断每个词是不是 <PAD>\n",
    "    mask = data == zidian_x['<PAD>']\n",
    "    #mask = torch.tensor([0,0,0,1,1])\n",
    "    # [b,50] -> [b,1,1,50]\n",
    "    mask = mask.reshape(-1,1,1,50)\n",
    "\n",
    "    # 在计算注意力时 是计算50个词和50个词相互之间的注意力  所以是个50*50的矩阵\n",
    "    # 是pad的列是true 意味着任何词对pad的注意力都是0\n",
    "    # 但是pad本身对其他词的注意力并不是0\n",
    "    # 所以是pad的行不是 true\n",
    "\n",
    "    #复制n次\n",
    "\n",
    "    #[b,1,1,50] -> [b,1,50,50]\n",
    "    mask = mask.expand(-1,1,50,50)  ### 通过 expand 构建pad mask矩阵\n",
    "\n",
    "    return mask\n",
    "\n",
    "def mask_tril(data):\n",
    "    # b句话  每句话50个词  这里是还没有embed的\n",
    "    # data = [b,50]\n",
    "\n",
    "    # 50*50 的矩阵表示每个词对其他词是否可见\n",
    "    # 上三角矩阵 不包括对角线  意味着每个词而言  他只能看到他自己  和他之前的词  而看不到之后的词\n",
    "    tril = 1 - torch.tril(torch.ones(1, 50, 50, dtype=torch.long))\n",
    "    # 判断y当中每个词是不是pad,如果是pad则不可见\n",
    "    # [b, 50]\n",
    "    mask = data == zidian_y['<PAD>']\n",
    "\n",
    "    # 变形+转型,为了之后的计算\n",
    "    # [b, 1, 50]\n",
    "    mask = mask.unsqueeze(1).long()\n",
    "\n",
    "    # mask和tril求并集\n",
    "    # [b, 1, 50] + [1, 50, 50] -> [b, 50, 50]\n",
    "    mask = mask + tril\n",
    "\n",
    "    # 转布尔型\n",
    "    mask = mask > 0\n",
    "\n",
    "    # 转布尔型,增加一个维度,便于后续的计算\n",
    "    mask = (mask == 1).unsqueeze(dim=1)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器层\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mh = MutilHead()\n",
    "        self.fc = FullyConnectOutput()\n",
    "    def forward(self,x,mask):\n",
    "        # 计算自注意力  维度不变\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        score = self.mh(x,x,x,mask) # 三个x 代表 Q K V\n",
    "        #全连接输出  维度不变 \n",
    "        # [b, 50, 32] - > [b,50,32]\n",
    "        out = self.fc(score)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = EncoderLayer()\n",
    "        self.layer_2 = EncoderLayer()\n",
    "        self.layer_3 = EncoderLayer()\n",
    "    def forward(self,x,mask):\n",
    "        x = self.layer_1(x,mask)\n",
    "        x = self.layer_2(x,mask)\n",
    "        x = self.layer_3(x,mask)\n",
    "        return x\n",
    "\n",
    "# 解码器层\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mh1 = MutilHead()\n",
    "        self.mh2 = MutilHead()\n",
    "        self.fc = FullyConnectOutput()\n",
    "\n",
    "    def forward(self,x,y,mask_pad_x,mask_tril_y):\n",
    "        # 先计算y的自注意力 维度不变\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        y = self.mh1(y,y,y,mask_tril_y)\n",
    "\n",
    "        # 结合x 和 y的注意力计算  维度不变\n",
    "        # [b,50,32],[b,50,32] -> [b,50,32]\n",
    "        y = self.mh2(y,x,x,mask_pad_x)\n",
    "\n",
    "        # 全连接输出 维度不变\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = DecoderLayer()\n",
    "        self.layer_2 = DecoderLayer()\n",
    "        self.layer_3 = DecoderLayer()\n",
    "\n",
    "    def forward(self,x,y,mask_pad_x,mask_tril_y):\n",
    "        y = self.layer_1(x,y,mask_pad_x,mask_tril_y)\n",
    "        y = self.layer_2(x,y,mask_pad_x,mask_tril_y)\n",
    "        y = self.layer_3(x,y,mask_pad_x,mask_tril_y)\n",
    "        return y\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_x = PositionEmbedding()\n",
    "        self.embed_y = PositionEmbedding()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.fc_out = torch.nn.Linear(32,39)\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        # [b, 1, 50, 50]\n",
    "        mask_pad_x = mask_pad(x)\n",
    "        mask_tril_y = mask_tril(y)\n",
    "        # 编码并且添加位置信息\n",
    "        # x = [b,50] -> [b,50,32]\n",
    "        # y = [b,50] -> [b,50,32]\n",
    "        x,y = self.embed_x(x),self.embed_y(y)\n",
    "\n",
    "        # 编码层计算\n",
    "        # [b,50,32] -> [b,50,32]\n",
    "        x = self.encoder(x,mask_pad_x)\n",
    "\n",
    "        # 解码层计算\n",
    "        # [b,50,32],[b,50,32] -> [b,50,32]\n",
    "        y = self.decoder(x,y,mask_pad_x,mask_tril_y)  # 用 tril_y 这种mask  代表了\n",
    "\n",
    "        #全连接层输出  维度不变\n",
    "        # [b,50,32]->[b,50,39]  # b句话 每句话50个词  39代表每个词属于词典里面39个词的概率值\n",
    "\n",
    "        y = self.fc_out(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.0001 nan 0.0\n",
      "0 200 0.0001 nan 0.0\n",
      "0 400 0.0001 nan 0.0\n",
      "0 600 0.0001 nan 0.0\n",
      "0 800 0.0001 nan 0.0\n",
      "0 1000 0.0001 nan 0.0\n",
      "0 1200 0.0001 nan 0.0\n",
      "0 1400 0.0001 nan 0.0\n",
      "0 1600 0.0001 nan 0.0\n",
      "0 1800 0.0001 nan 0.0\n",
      "0 2000 0.0001 nan 0.0\n",
      "0 2200 0.0001 nan 0.0\n",
      "0 2400 0.0001 nan 0.0\n",
      "0 2600 0.0001 nan 0.0\n",
      "0 2800 0.0001 nan 0.0\n",
      "0 3000 0.0001 nan 0.0\n",
      "0\n",
      "<SOS>uv7jfcxgfhkxn6kxup77kkhfcmm1xpclmmzmn<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>NNMZMMLCPX8MMCFHKK22PUXK3NXKHFGXCFJ2VU<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "1\n",
      "<SOS>j6mmxva9nosgc7kh8zbmvcnvhhdno5c<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>CC4ONDHHVNCVMBZ1HK2CGSON0AVXMM3J<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "2\n",
      "<SOS>8fzkbpgnjmkm8k9pf8bynobpfvvmj9<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>00JMVVFPBONYB1FP0K1MKMJNGPBKZF1<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "3\n",
      "<SOS>vzfimxhmgo3zrmnawhcmcb676g1lkcj8b<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>BB1JCKL8G323BCMCHWANMRZ6OGMHXMIFZV<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "4\n",
      "<SOS>iypafdzufsmljc6zbczbsbsxyxgxf3buiszonznhsba<EOS><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>AABSHNZNOZSIUB6FXGXYXSBSBZCBZ3CJLMSFUZDFAPYI<EOS><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "5\n",
      "<SOS>b7hobqzssfd8lc52m9kfbd7kxkvbhlik4f<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>FF5KILHBVKXK2DBFK0M74CL1DFSSZQBOH2B<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "6\n",
      "<SOS>6enh4zczzaf9jfizxb5kxcmc7zkb7z4mb8mdmea<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>AAEMDM1BM5Z2BKZ2CMCXK4BXZIFJ0FAZZCZ5HNE3<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n",
      "7\n",
      "<SOS>pnp99f7k6zbmlzxfjtxrnjwox7vmnx8pbcc5piv<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>VVIP4CCBP1XNMV2XOWJNRXTJFXZLMBZ3K2F00PNP<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS><SOS>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 预测函数\n",
    "def predict(x):\n",
    "    # x = [1, 50]\n",
    "    model.eval()\n",
    "\n",
    "    # [1, 1, 50, 50]\n",
    "    mask_pad_x = mask_pad(x)\n",
    "\n",
    "    # 初始化输出,这个是固定值\n",
    "    # [1, 50]\n",
    "    # [[0,2,2,2...]]\n",
    "    target = [zidian_y['<SOS>']] + [zidian_y['<PAD>']] * 49\n",
    "    target = torch.LongTensor(target).unsqueeze(0)\n",
    "\n",
    "    # x编码,添加位置信息\n",
    "    # [1, 50] -> [1, 50, 32]\n",
    "    x = model.embed_x(x)\n",
    "\n",
    "    # 编码层计算,维度不变\n",
    "    # [1, 50, 32] -> [1, 50, 32]\n",
    "    x = model.encoder(x, mask_pad_x)\n",
    "\n",
    "    # 遍历生成第1个词到第49个词\n",
    "    for i in range(49):\n",
    "        # [1, 50]\n",
    "        y = target\n",
    "\n",
    "        # [1, 1, 50, 50]\n",
    "        mask_tril_y = mask_tril(y)\n",
    "\n",
    "        # y编码,添加位置信息\n",
    "        # [1, 50] -> [1, 50, 32]\n",
    "        y = model.embed_y(y)\n",
    "\n",
    "        # 解码层计算,维度不变\n",
    "        # [1, 50, 32],[1, 50, 32] -> [1, 50, 32]\n",
    "        y = model.decoder(x, y, mask_pad_x, mask_tril_y)\n",
    "\n",
    "        # 全连接输出,39分类\n",
    "        # [1, 50, 32] -> [1, 50, 39]\n",
    "        out = model.fc_out(y)\n",
    "\n",
    "        # 取出当前词的输出\n",
    "        # [1, 50, 39] -> [1, 39]\n",
    "        out = out[:, i, :]\n",
    "\n",
    "        # 取出分类结果\n",
    "        # [1, 39] -> [1]\n",
    "        out = out.argmax(dim=1).detach()\n",
    "\n",
    "        # 以当前词预测下一个词,填到结果中\n",
    "        target[:, i + 1] = out\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        # x = [8, 50]\n",
    "        # y = [8, 51]\n",
    "\n",
    "        # 在训练时,是拿y的每一个字符输入,预测下一个字符,所以不需要最后一个字\n",
    "        # [8, 50, 39]\n",
    "        pred = model(x, y[:, :-1])\n",
    "\n",
    "        # [8, 50, 39] -> [400, 39]\n",
    "        pred = pred.reshape(-1, 39)\n",
    "\n",
    "        # [8, 51] -> [400]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "\n",
    "        # 忽略pad\n",
    "        select = y != zidian_y['<PAD>']\n",
    "        pred = pred[select]\n",
    "        y = y[select]\n",
    "\n",
    "        loss = loss_func(pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            # [select, 39] -> [select]\n",
    "            pred = pred.argmax(1)\n",
    "            correct = (pred == y).sum().item()\n",
    "            accuracy = correct / len(pred)\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(epoch, i, lr, loss.item(), accuracy)\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "# 测试\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    print(''.join([zidian_xr[i] for i in x[i].tolist()]))\n",
    "    print(''.join([zidian_yr[i] for i in y[i].tolist()]))\n",
    "    print(''.join([zidian_yr[i] for i in predict(x[i].unsqueeze(0))[0].tolist()]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bdac110bc2195a97d8863c350d20b7f4e667502377256e4f50b096594c943fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
