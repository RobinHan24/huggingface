{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e6b7bbeb142b297d7b8c31d77afb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d608d938c24cd78fbc0b0875473682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bf86cc2c4142feb1cb2261e99ee80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
       " ['选择珠江花园的原因就是方便。',\n",
       "  '笔记本的键盘确实爽。',\n",
       "  '房间太小。其他的都一般。',\n",
       "  '今天才知道这书还有第6卷,真有点郁闷.',\n",
       "  '机器背面似乎被撕了张什么标签，残胶还在。'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer # 每个模型都有其对应的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese',\n",
    "cache_dir = None,\n",
    "force_download=False)\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]\n",
    "tokenizer,sents\n",
    "'''\n",
    "tokenizer 具体参数\n",
    "vocab_size=21128,   词汇数量 \n",
    "model_max_len=512,  句子最大长度\n",
    "is_fast=False,\n",
    "padding_side='right',  向右补全\n",
    "truncation_side='right',  右边截断\n",
    "special_tokens={\n",
    "'unk_token': '[UNK]', 未知的token 用 UNK表示\n",
    "'sep_token': '[SEP]',  结束token\n",
    "'pad_token': '[PAD]',  padding \n",
    "'cls_token': '[CLS]', 开始\n",
    "'mask_token': '[MASK]'}),  掩膜\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer.encode(  # encode 完之后字符变成了数字，即在词汇表中的索引\n",
    "    text = sents[0],\n",
    "    text_pair = sents[1],\n",
    "    #\"当句子长度大于max_length 时 截断\"\n",
    "    truncation = True,\n",
    "    # 一律补pad到maxx_length长度\n",
    "    padding = 'max_length',\n",
    "    add_special_tokens = True,\n",
    "    max_length = 30,\n",
    "    return_tensors=None\n",
    ") \n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out) # 解码之后就是原始的句子 + 头尾符号和padding  bertchinese 每个字作为一个词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'length': 30}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n{'input_ids': [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, \\n5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], \\n'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], \\n'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], \\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],  \\n'length': 30}\\ninput_ids:编码后的词\\ntoken_type_ids：第一个句子 [CLS XXX SEP]和[PAD]的位置是0  第二个句子[XXX SEP]的位置是1  \\nspecial_tokens_mask:特殊符号的位置是1 其他位置是0 \\nattention_mask ： paddinng 位置是0  其他位置是1\\nlength ：句子长度是30\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####增强编码\n",
    "out = tokenizer.encode_plus(\n",
    "    text = sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    # 当句子长度大于max_length 时 截断\n",
    "        truncation = True,\n",
    "    # 一律补pad到maxx_length长度\n",
    "    padding = 'max_length',\n",
    "    add_special_tokens = True,\n",
    "    max_length = 30,\n",
    "    return_tensors=None,# 默认返回是 list\n",
    "    \n",
    "    return_token_type_ids =True, # 返回 token_type_ids\n",
    "    return_attention_mask =True, # 返回attention mask\n",
    "    return_special_tokens_mask=True, # 返回特殊符号标识\n",
    "    return_length=True\n",
    "\n",
    ")\n",
    "print (out)\n",
    "'''\n",
    "{'input_ids': [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, \n",
    "5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], \n",
    "'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], \n",
    "'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], \n",
    "'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],  \n",
    "'length': 30}\n",
    "input_ids:编码后的词\n",
    "token_type_ids：第一个句子 [CLS XXX SEP]和[PAD]的位置是0  第二个句子[XXX SEP]的位置是1  \n",
    "special_tokens_mask:特殊符号的位置是1 其他位置是0 \n",
    "attention_mask ： paddinng 位置是0  其他位置是1\n",
    "length ：句子长度是30\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "length : 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in out.items():\n",
    "    print (k,':',v)\n",
    "tokenizer.decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "length : [15, 12]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 [SEP]',\n",
       " '[CLS] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#批量编码一对句子   \n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1]],\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "\n",
    "    #可取值tf,pt,np,默认为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #返回token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #返回attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #返回length 标识长度\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "#input_ids 就是编码后的词\n",
    "#token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
    "#attention_mask pad的位置是0,其他位置是1\n",
    "#length 返回句子长度\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "length : [27, 30]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#批量编码多对句子   \n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]),(sents[2], sents[3])],\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "\n",
    "    #可取值tf,pt,np,默认为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    #返回token_type_ids\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    #返回attention_mask\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    #返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
    "    #return_offsets_mapping=True,\n",
    "\n",
    "    #返回length 标识长度\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "#input_ids 就是编码后的词\n",
    "#token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
    "#attention_mask pad的位置是0,其他位置是1\n",
    "#length 返回句子长度\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out['input_ids'][0])\n",
    "# tokenizer.decode(out['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21128, False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取字典\n",
    "zidian = tokenizer.get_vocab()\n",
    "type(zidian),len(zidian),'月光' in zidian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 21131, 21128, 21130)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加新词\n",
    "tokenizer.add_tokens(new_tokens=['月光','希望'])\n",
    "# 添加新符号\n",
    "tokenizer.add_special_tokens({'eos_token':'[EOS]'})\n",
    "zidian = tokenizer.get_vocab()  # 字典内容  词语：编号\n",
    "type(zidian),len(zidian),zidian['月光'],zidian['[EOS]']   #字典的数量增加了3 月光,希望，'[EOS] 被依次添加在字典最后面\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zidian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 月光 的 新 希望 [EOS] [SEP] [PAD]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码新添加的词语\n",
    "out = tokenizer.encode(  # encode 完之后字符变成了数字，即在词汇表中的索引\n",
    "    text = '月光的新希望[EOS]',\n",
    "    text_pair = None,\n",
    "    #\"当句子长度大于max_length 时 截断\"\n",
    "    truncation = True,\n",
    "    # 一律补pad到maxx_length长度\n",
    "    padding = 'max_length',\n",
    "    add_special_tokens = True,\n",
    "    max_length = 8,\n",
    "    return_tensors=None\n",
    ") \n",
    "out\n",
    "tokenizer.decode(out)\n",
    "'''\n",
    "没有添新词的时候 ‘月光’ 两个字是被分开编码的   添加新词之后‘月光’作为一个整体被编码\n",
    "'[CLS] 月光 的 新 希望 [EOS] [SEP] [PAD]'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bdac110bc2195a97d8863c350d20b7f4e667502377256e4f50b096594c943fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
